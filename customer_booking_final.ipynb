{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba9372a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e178148c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       num_passengers sales_channel  trip_type  purchase_lead  length_of_stay  \\\n",
      "0                   2      Internet  RoundTrip            262              19   \n",
      "1                   1      Internet  RoundTrip            112              20   \n",
      "2                   2      Internet  RoundTrip            243              22   \n",
      "3                   1      Internet  RoundTrip             96              31   \n",
      "4                   2      Internet  RoundTrip             68              22   \n",
      "...               ...           ...        ...            ...             ...   \n",
      "49995               2      Internet  RoundTrip             27               6   \n",
      "49996               1      Internet  RoundTrip            111               6   \n",
      "49997               1      Internet  RoundTrip             24               6   \n",
      "49998               1      Internet  RoundTrip             15               6   \n",
      "49999               1      Internet  RoundTrip             19               6   \n",
      "\n",
      "       flight_hour flight_day   route booking_origin  wants_extra_baggage  \\\n",
      "0                7        Sat  AKLDEL    New Zealand                    1   \n",
      "1                3        Sat  AKLDEL    New Zealand                    0   \n",
      "2               17        Wed  AKLDEL          India                    1   \n",
      "3                4        Sat  AKLDEL    New Zealand                    0   \n",
      "4               15        Wed  AKLDEL          India                    1   \n",
      "...            ...        ...     ...            ...                  ...   \n",
      "49995            9        Sat  PERPNH      Australia                    1   \n",
      "49996            4        Sun  PERPNH      Australia                    0   \n",
      "49997           22        Sat  PERPNH      Australia                    0   \n",
      "49998           11        Mon  PERPNH      Australia                    1   \n",
      "49999           10        Thu  PERPNH      Australia                    0   \n",
      "\n",
      "       wants_preferred_seat  wants_in_flight_meals  flight_duration  \\\n",
      "0                         0                      0             5.52   \n",
      "1                         0                      0             5.52   \n",
      "2                         1                      0             5.52   \n",
      "3                         0                      1             5.52   \n",
      "4                         0                      1             5.52   \n",
      "...                     ...                    ...              ...   \n",
      "49995                     0                      1             5.62   \n",
      "49996                     0                      0             5.62   \n",
      "49997                     0                      1             5.62   \n",
      "49998                     0                      1             5.62   \n",
      "49999                     1                      0             5.62   \n",
      "\n",
      "       booking_complete  \n",
      "0                     0  \n",
      "1                     0  \n",
      "2                     0  \n",
      "3                     0  \n",
      "4                     0  \n",
      "...                 ...  \n",
      "49995                 0  \n",
      "49996                 0  \n",
      "49997                 0  \n",
      "49998                 0  \n",
      "49999                 0  \n",
      "\n",
      "[50000 rows x 14 columns]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(50000, 14)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the CSV file\n",
    "data = pd.read_csv('customer_booking.csv', encoding='latin-1')\n",
    "\n",
    "print(data)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "86973594",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate the features (X) and the target variable (y)\n",
    "X = data.drop('booking_complete', axis=1)\n",
    "y = data['booking_complete']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "52e02d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the categorical columns\n",
    "categorical_columns = X.select_dtypes(include=['object']).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1f9d2f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform one-hot encoding on categorical variables\n",
    "encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "X_encoded = encoder.fit_transform(X[categorical_columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7503e998",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the unique feature names for the encoded columns\n",
    "feature_names = []\n",
    "for feature in encoder.categories_:\n",
    "    feature_names.extend(feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f7e949c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame from the encoded columns\n",
    "X_encoded_df = pd.DataFrame(X_encoded, columns=feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d2239a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the original categorical columns from X\n",
    "X = X.drop(categorical_columns, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7cc5b0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate the encoded categorical columns with the remaining numerical columns\n",
    "X_processed = pd.concat([X, X_encoded_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d4c6e9ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_processed, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e8557e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a random forest classifier\n",
    "clf = RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b58211b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestClassifier()"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the classifier\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3805cf62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the test set\n",
    "y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cb369397",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8543\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the accuracy of the classifier\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ee630857",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;background-color: white;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=4, estimator=LogisticRegression(max_iter=1000), n_jobs=-1,\n",
       "             param_grid={&#x27;class_weight&#x27;: [{0: 1, 1: 1}, {0: 1, 1: 2},\n",
       "                                          {0: 1, 1: 3}]})</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(cv=4, estimator=LogisticRegression(max_iter=1000), n_jobs=-1,\n",
       "             param_grid={&#x27;class_weight&#x27;: [{0: 1, 1: 1}, {0: 1, 1: 2},\n",
       "                                          {0: 1, 1: 3}]})</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(max_iter=1000)</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" ><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(max_iter=1000)</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(cv=4, estimator=LogisticRegression(max_iter=1000), n_jobs=-1,\n",
       "             param_grid={'class_weight': [{0: 1, 1: 1}, {0: 1, 1: 2},\n",
       "                                          {0: 1, 1: 3}]})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "grid = GridSearchCV(\n",
    "    estimator = LogisticRegression(max_iter=1000),\n",
    "    param_grid = {'class_weight': [{0: 1, 1: v} for v in range(1, 4)]},\n",
    "    cv=4,\n",
    "    n_jobs=-1\n",
    ")\n",
    "grid.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "12ddf3e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_class_weight</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.548621</td>\n",
       "      <td>0.112035</td>\n",
       "      <td>0.010721</td>\n",
       "      <td>0.006518</td>\n",
       "      <td>{0: 1, 1: 1}</td>\n",
       "      <td>{'class_weight': {0: 1, 1: 1}}</td>\n",
       "      <td>0.85048</td>\n",
       "      <td>0.85048</td>\n",
       "      <td>0.85040</td>\n",
       "      <td>0.85040</td>\n",
       "      <td>0.85044</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.507156</td>\n",
       "      <td>0.133786</td>\n",
       "      <td>0.009474</td>\n",
       "      <td>0.004911</td>\n",
       "      <td>{0: 1, 1: 2}</td>\n",
       "      <td>{'class_weight': {0: 1, 1: 2}}</td>\n",
       "      <td>0.85048</td>\n",
       "      <td>0.85048</td>\n",
       "      <td>0.85040</td>\n",
       "      <td>0.84872</td>\n",
       "      <td>0.85002</td>\n",
       "      <td>0.000751</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.574248</td>\n",
       "      <td>0.164522</td>\n",
       "      <td>0.010721</td>\n",
       "      <td>0.004750</td>\n",
       "      <td>{0: 1, 1: 3}</td>\n",
       "      <td>{'class_weight': {0: 1, 1: 3}}</td>\n",
       "      <td>0.84696</td>\n",
       "      <td>0.82144</td>\n",
       "      <td>0.82984</td>\n",
       "      <td>0.80016</td>\n",
       "      <td>0.82460</td>\n",
       "      <td>0.016843</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "0       0.548621      0.112035         0.010721        0.006518   \n",
       "1       0.507156      0.133786         0.009474        0.004911   \n",
       "2       0.574248      0.164522         0.010721        0.004750   \n",
       "\n",
       "  param_class_weight                          params  split0_test_score  \\\n",
       "0       {0: 1, 1: 1}  {'class_weight': {0: 1, 1: 1}}            0.85048   \n",
       "1       {0: 1, 1: 2}  {'class_weight': {0: 1, 1: 2}}            0.85048   \n",
       "2       {0: 1, 1: 3}  {'class_weight': {0: 1, 1: 3}}            0.84696   \n",
       "\n",
       "   split1_test_score  split2_test_score  split3_test_score  mean_test_score  \\\n",
       "0            0.85048            0.85040            0.85040          0.85044   \n",
       "1            0.85048            0.85040            0.84872          0.85002   \n",
       "2            0.82144            0.82984            0.80016          0.82460   \n",
       "\n",
       "   std_test_score  rank_test_score  \n",
       "0        0.000040                1  \n",
       "1        0.000751                2  \n",
       "2        0.016843                3  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(grid.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cb86ac69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-4 {color: black;background-color: white;}#sk-container-id-4 pre{padding: 0;}#sk-container-id-4 div.sk-toggleable {background-color: white;}#sk-container-id-4 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-4 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-4 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-4 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-4 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-4 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-4 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-4 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-4 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-4 div.sk-item {position: relative;z-index: 1;}#sk-container-id-4 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-4 div.sk-item::before, #sk-container-id-4 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-4 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-4 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-4 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-4 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-4 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-4 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-4 div.sk-label-container {text-align: center;}#sk-container-id-4 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-4 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-4\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=10, estimator=LogisticRegression(max_iter=1000), n_jobs=-1,\n",
       "             param_grid={&#x27;class_weight&#x27;: [{0: 1, 1: 1.0},\n",
       "                                          {0: 1, 1: 1.6551724137931034},\n",
       "                                          {0: 1, 1: 2.310344827586207},\n",
       "                                          {0: 1, 1: 2.9655172413793105},\n",
       "                                          {0: 1, 1: 3.6206896551724137},\n",
       "                                          {0: 1, 1: 4.275862068965517},\n",
       "                                          {0: 1, 1: 4.931034482758621},\n",
       "                                          {0: 1, 1: 5.586206896551724},\n",
       "                                          {0: 1, 1: 6.241379310344827},\n",
       "                                          {0: 1, 1: 6.896551724137931},...\n",
       "                                          {0: 1, 1: 14.758620689655173},\n",
       "                                          {0: 1, 1: 15.413793103448276},\n",
       "                                          {0: 1, 1: 16.06896551724138},\n",
       "                                          {0: 1, 1: 16.724137931034484},\n",
       "                                          {0: 1, 1: 17.379310344827587},\n",
       "                                          {0: 1, 1: 18.03448275862069},\n",
       "                                          {0: 1, 1: 18.689655172413794},\n",
       "                                          {0: 1, 1: 19.344827586206897},\n",
       "                                          {0: 1, 1: 20.0}]},\n",
       "             refit=&#x27;precision&#x27;, return_train_score=True,\n",
       "             scoring={&#x27;precision&#x27;: make_scorer(precision_score),\n",
       "                      &#x27;recall_score&#x27;: make_scorer(recall_score)})</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" ><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(cv=10, estimator=LogisticRegression(max_iter=1000), n_jobs=-1,\n",
       "             param_grid={&#x27;class_weight&#x27;: [{0: 1, 1: 1.0},\n",
       "                                          {0: 1, 1: 1.6551724137931034},\n",
       "                                          {0: 1, 1: 2.310344827586207},\n",
       "                                          {0: 1, 1: 2.9655172413793105},\n",
       "                                          {0: 1, 1: 3.6206896551724137},\n",
       "                                          {0: 1, 1: 4.275862068965517},\n",
       "                                          {0: 1, 1: 4.931034482758621},\n",
       "                                          {0: 1, 1: 5.586206896551724},\n",
       "                                          {0: 1, 1: 6.241379310344827},\n",
       "                                          {0: 1, 1: 6.896551724137931},...\n",
       "                                          {0: 1, 1: 14.758620689655173},\n",
       "                                          {0: 1, 1: 15.413793103448276},\n",
       "                                          {0: 1, 1: 16.06896551724138},\n",
       "                                          {0: 1, 1: 16.724137931034484},\n",
       "                                          {0: 1, 1: 17.379310344827587},\n",
       "                                          {0: 1, 1: 18.03448275862069},\n",
       "                                          {0: 1, 1: 18.689655172413794},\n",
       "                                          {0: 1, 1: 19.344827586206897},\n",
       "                                          {0: 1, 1: 20.0}]},\n",
       "             refit=&#x27;precision&#x27;, return_train_score=True,\n",
       "             scoring={&#x27;precision&#x27;: make_scorer(precision_score),\n",
       "                      &#x27;recall_score&#x27;: make_scorer(recall_score)})</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-7\" type=\"checkbox\" ><label for=\"sk-estimator-id-7\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(max_iter=1000)</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-8\" type=\"checkbox\" ><label for=\"sk-estimator-id-8\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(max_iter=1000)</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(cv=10, estimator=LogisticRegression(max_iter=1000), n_jobs=-1,\n",
       "             param_grid={'class_weight': [{0: 1, 1: 1.0},\n",
       "                                          {0: 1, 1: 1.6551724137931034},\n",
       "                                          {0: 1, 1: 2.310344827586207},\n",
       "                                          {0: 1, 1: 2.9655172413793105},\n",
       "                                          {0: 1, 1: 3.6206896551724137},\n",
       "                                          {0: 1, 1: 4.275862068965517},\n",
       "                                          {0: 1, 1: 4.931034482758621},\n",
       "                                          {0: 1, 1: 5.586206896551724},\n",
       "                                          {0: 1, 1: 6.241379310344827},\n",
       "                                          {0: 1, 1: 6.896551724137931},...\n",
       "                                          {0: 1, 1: 14.758620689655173},\n",
       "                                          {0: 1, 1: 15.413793103448276},\n",
       "                                          {0: 1, 1: 16.06896551724138},\n",
       "                                          {0: 1, 1: 16.724137931034484},\n",
       "                                          {0: 1, 1: 17.379310344827587},\n",
       "                                          {0: 1, 1: 18.03448275862069},\n",
       "                                          {0: 1, 1: 18.689655172413794},\n",
       "                                          {0: 1, 1: 19.344827586206897},\n",
       "                                          {0: 1, 1: 20.0}]},\n",
       "             refit='precision', return_train_score=True,\n",
       "             scoring={'precision': make_scorer(precision_score),\n",
       "                      'recall_score': make_scorer(recall_score)})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import precision_score, recall_score, make_scorer\n",
    "import numpy as np\n",
    "\n",
    "grid = GridSearchCV(\n",
    "    estimator = LogisticRegression(max_iter=1000),\n",
    "    param_grid = {'class_weight': [{0: 1, 1: v} for v in np.linspace(1, 20, 30)]},\n",
    "    scoring={'precision': make_scorer(precision_score), 'recall_score': make_scorer(recall_score)},\n",
    "    refit='precision',\n",
    "    return_train_score=True,\n",
    "    cv=10,\n",
    "    n_jobs=-1\n",
    ")\n",
    "grid.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ca3189e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_class_weight</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_precision</th>\n",
       "      <th>split1_test_precision</th>\n",
       "      <th>split2_test_precision</th>\n",
       "      <th>split3_test_precision</th>\n",
       "      <th>...</th>\n",
       "      <th>split2_train_recall_score</th>\n",
       "      <th>split3_train_recall_score</th>\n",
       "      <th>split4_train_recall_score</th>\n",
       "      <th>split5_train_recall_score</th>\n",
       "      <th>split6_train_recall_score</th>\n",
       "      <th>split7_train_recall_score</th>\n",
       "      <th>split8_train_recall_score</th>\n",
       "      <th>split9_train_recall_score</th>\n",
       "      <th>mean_train_recall_score</th>\n",
       "      <th>std_train_recall_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.761607</td>\n",
       "      <td>0.145236</td>\n",
       "      <td>0.019650</td>\n",
       "      <td>0.004624</td>\n",
       "      <td>{0: 1, 1: 1.0}</td>\n",
       "      <td>{'class_weight': {0: 1, 1: 1.0}}</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.617786</td>\n",
       "      <td>0.175277</td>\n",
       "      <td>0.017653</td>\n",
       "      <td>0.003813</td>\n",
       "      <td>{0: 1, 1: 1.6551724137931034}</td>\n",
       "      <td>{'class_weight': {0: 1, 1: 1.6551724137931034}}</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.874075</td>\n",
       "      <td>0.193313</td>\n",
       "      <td>0.019148</td>\n",
       "      <td>0.006193</td>\n",
       "      <td>{0: 1, 1: 2.310344827586207}</td>\n",
       "      <td>{'class_weight': {0: 1, 1: 2.310344827586207}}</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.304348</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009955</td>\n",
       "      <td>0.018128</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004160</td>\n",
       "      <td>0.020208</td>\n",
       "      <td>0.016790</td>\n",
       "      <td>0.021694</td>\n",
       "      <td>0.011887</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>0.007844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.912075</td>\n",
       "      <td>0.197343</td>\n",
       "      <td>0.023637</td>\n",
       "      <td>0.008079</td>\n",
       "      <td>{0: 1, 1: 2.9655172413793105}</td>\n",
       "      <td>{'class_weight': {0: 1, 1: 2.9655172413793105}}</td>\n",
       "      <td>0.161290</td>\n",
       "      <td>0.325000</td>\n",
       "      <td>0.288889</td>\n",
       "      <td>0.236287</td>\n",
       "      <td>...</td>\n",
       "      <td>0.063001</td>\n",
       "      <td>0.091233</td>\n",
       "      <td>0.054532</td>\n",
       "      <td>0.050817</td>\n",
       "      <td>0.085587</td>\n",
       "      <td>0.079643</td>\n",
       "      <td>0.088410</td>\n",
       "      <td>0.071471</td>\n",
       "      <td>0.077040</td>\n",
       "      <td>0.016527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.881650</td>\n",
       "      <td>0.186410</td>\n",
       "      <td>0.024535</td>\n",
       "      <td>0.009966</td>\n",
       "      <td>{0: 1, 1: 3.6206896551724137}</td>\n",
       "      <td>{'class_weight': {0: 1, 1: 3.6206896551724137}}</td>\n",
       "      <td>0.163142</td>\n",
       "      <td>0.237288</td>\n",
       "      <td>0.226601</td>\n",
       "      <td>0.177436</td>\n",
       "      <td>...</td>\n",
       "      <td>0.189153</td>\n",
       "      <td>0.251857</td>\n",
       "      <td>0.223477</td>\n",
       "      <td>0.172660</td>\n",
       "      <td>0.235661</td>\n",
       "      <td>0.223477</td>\n",
       "      <td>0.233581</td>\n",
       "      <td>0.215156</td>\n",
       "      <td>0.221182</td>\n",
       "      <td>0.024179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.008922</td>\n",
       "      <td>0.230364</td>\n",
       "      <td>0.028424</td>\n",
       "      <td>0.013654</td>\n",
       "      <td>{0: 1, 1: 4.275862068965517}</td>\n",
       "      <td>{'class_weight': {0: 1, 1: 4.275862068965517}}</td>\n",
       "      <td>0.158451</td>\n",
       "      <td>0.229457</td>\n",
       "      <td>0.295688</td>\n",
       "      <td>0.170423</td>\n",
       "      <td>...</td>\n",
       "      <td>0.316048</td>\n",
       "      <td>0.357949</td>\n",
       "      <td>0.349777</td>\n",
       "      <td>0.322883</td>\n",
       "      <td>0.355126</td>\n",
       "      <td>0.349331</td>\n",
       "      <td>0.353343</td>\n",
       "      <td>0.338336</td>\n",
       "      <td>0.345413</td>\n",
       "      <td>0.015530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.907042</td>\n",
       "      <td>0.145782</td>\n",
       "      <td>0.020148</td>\n",
       "      <td>0.006383</td>\n",
       "      <td>{0: 1, 1: 4.931034482758621}</td>\n",
       "      <td>{'class_weight': {0: 1, 1: 4.931034482758621}}</td>\n",
       "      <td>0.157895</td>\n",
       "      <td>0.235052</td>\n",
       "      <td>0.266537</td>\n",
       "      <td>0.174094</td>\n",
       "      <td>...</td>\n",
       "      <td>0.454532</td>\n",
       "      <td>0.449926</td>\n",
       "      <td>0.438187</td>\n",
       "      <td>0.428678</td>\n",
       "      <td>0.465230</td>\n",
       "      <td>0.463150</td>\n",
       "      <td>0.464487</td>\n",
       "      <td>0.454978</td>\n",
       "      <td>0.457995</td>\n",
       "      <td>0.015775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.904527</td>\n",
       "      <td>0.149949</td>\n",
       "      <td>0.019249</td>\n",
       "      <td>0.004528</td>\n",
       "      <td>{0: 1, 1: 5.586206896551724}</td>\n",
       "      <td>{'class_weight': {0: 1, 1: 5.586206896551724}}</td>\n",
       "      <td>0.158498</td>\n",
       "      <td>0.210526</td>\n",
       "      <td>0.237172</td>\n",
       "      <td>0.171818</td>\n",
       "      <td>...</td>\n",
       "      <td>0.592719</td>\n",
       "      <td>0.572511</td>\n",
       "      <td>0.583358</td>\n",
       "      <td>0.570728</td>\n",
       "      <td>0.575632</td>\n",
       "      <td>0.572660</td>\n",
       "      <td>0.576672</td>\n",
       "      <td>0.573700</td>\n",
       "      <td>0.580086</td>\n",
       "      <td>0.008402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.903327</td>\n",
       "      <td>0.139174</td>\n",
       "      <td>0.028009</td>\n",
       "      <td>0.009290</td>\n",
       "      <td>{0: 1, 1: 6.241379310344827}</td>\n",
       "      <td>{'class_weight': {0: 1, 1: 6.241379310344827}}</td>\n",
       "      <td>0.159193</td>\n",
       "      <td>0.198522</td>\n",
       "      <td>0.219523</td>\n",
       "      <td>0.168973</td>\n",
       "      <td>...</td>\n",
       "      <td>0.680089</td>\n",
       "      <td>0.672957</td>\n",
       "      <td>0.671174</td>\n",
       "      <td>0.668648</td>\n",
       "      <td>0.659733</td>\n",
       "      <td>0.663150</td>\n",
       "      <td>0.661070</td>\n",
       "      <td>0.657801</td>\n",
       "      <td>0.669638</td>\n",
       "      <td>0.008789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.851733</td>\n",
       "      <td>0.091967</td>\n",
       "      <td>0.017154</td>\n",
       "      <td>0.004342</td>\n",
       "      <td>{0: 1, 1: 6.896551724137931}</td>\n",
       "      <td>{'class_weight': {0: 1, 1: 6.896551724137931}}</td>\n",
       "      <td>0.162146</td>\n",
       "      <td>0.185014</td>\n",
       "      <td>0.201855</td>\n",
       "      <td>0.165546</td>\n",
       "      <td>...</td>\n",
       "      <td>0.758841</td>\n",
       "      <td>0.726449</td>\n",
       "      <td>0.754829</td>\n",
       "      <td>0.741753</td>\n",
       "      <td>0.728678</td>\n",
       "      <td>0.727786</td>\n",
       "      <td>0.731947</td>\n",
       "      <td>0.737741</td>\n",
       "      <td>0.738641</td>\n",
       "      <td>0.010877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.950211</td>\n",
       "      <td>0.199103</td>\n",
       "      <td>0.018951</td>\n",
       "      <td>0.005815</td>\n",
       "      <td>{0: 1, 1: 7.551724137931034}</td>\n",
       "      <td>{'class_weight': {0: 1, 1: 7.551724137931034}}</td>\n",
       "      <td>0.166584</td>\n",
       "      <td>0.170968</td>\n",
       "      <td>0.185162</td>\n",
       "      <td>0.162959</td>\n",
       "      <td>...</td>\n",
       "      <td>0.829421</td>\n",
       "      <td>0.798811</td>\n",
       "      <td>0.838336</td>\n",
       "      <td>0.821842</td>\n",
       "      <td>0.804012</td>\n",
       "      <td>0.802080</td>\n",
       "      <td>0.804012</td>\n",
       "      <td>0.815750</td>\n",
       "      <td>0.812428</td>\n",
       "      <td>0.013328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.972725</td>\n",
       "      <td>0.204311</td>\n",
       "      <td>0.023537</td>\n",
       "      <td>0.012053</td>\n",
       "      <td>{0: 1, 1: 8.206896551724139}</td>\n",
       "      <td>{'class_weight': {0: 1, 1: 8.206896551724139}}</td>\n",
       "      <td>0.164053</td>\n",
       "      <td>0.161616</td>\n",
       "      <td>0.170384</td>\n",
       "      <td>0.156173</td>\n",
       "      <td>...</td>\n",
       "      <td>0.880832</td>\n",
       "      <td>0.862110</td>\n",
       "      <td>0.871322</td>\n",
       "      <td>0.881872</td>\n",
       "      <td>0.858692</td>\n",
       "      <td>0.859287</td>\n",
       "      <td>0.857504</td>\n",
       "      <td>0.867905</td>\n",
       "      <td>0.866527</td>\n",
       "      <td>0.009062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.992364</td>\n",
       "      <td>0.257309</td>\n",
       "      <td>0.021142</td>\n",
       "      <td>0.006846</td>\n",
       "      <td>{0: 1, 1: 8.862068965517242}</td>\n",
       "      <td>{'class_weight': {0: 1, 1: 8.862068965517242}}</td>\n",
       "      <td>0.161142</td>\n",
       "      <td>0.158779</td>\n",
       "      <td>0.165002</td>\n",
       "      <td>0.153792</td>\n",
       "      <td>...</td>\n",
       "      <td>0.907132</td>\n",
       "      <td>0.900297</td>\n",
       "      <td>0.897771</td>\n",
       "      <td>0.911887</td>\n",
       "      <td>0.893908</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.893611</td>\n",
       "      <td>0.898811</td>\n",
       "      <td>0.899082</td>\n",
       "      <td>0.006172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1.043197</td>\n",
       "      <td>0.160256</td>\n",
       "      <td>0.021398</td>\n",
       "      <td>0.010140</td>\n",
       "      <td>{0: 1, 1: 9.517241379310345}</td>\n",
       "      <td>{'class_weight': {0: 1, 1: 9.517241379310345}}</td>\n",
       "      <td>0.156605</td>\n",
       "      <td>0.157830</td>\n",
       "      <td>0.162785</td>\n",
       "      <td>0.154265</td>\n",
       "      <td>...</td>\n",
       "      <td>0.926152</td>\n",
       "      <td>0.921545</td>\n",
       "      <td>0.920654</td>\n",
       "      <td>0.930163</td>\n",
       "      <td>0.913967</td>\n",
       "      <td>0.917979</td>\n",
       "      <td>0.913967</td>\n",
       "      <td>0.919316</td>\n",
       "      <td>0.919572</td>\n",
       "      <td>0.005274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.860723</td>\n",
       "      <td>0.178716</td>\n",
       "      <td>0.022839</td>\n",
       "      <td>0.009036</td>\n",
       "      <td>{0: 1, 1: 10.172413793103448}</td>\n",
       "      <td>{'class_weight': {0: 1, 1: 10.172413793103448}}</td>\n",
       "      <td>0.157895</td>\n",
       "      <td>0.157023</td>\n",
       "      <td>0.158928</td>\n",
       "      <td>0.154208</td>\n",
       "      <td>...</td>\n",
       "      <td>0.943982</td>\n",
       "      <td>0.931055</td>\n",
       "      <td>0.931204</td>\n",
       "      <td>0.943685</td>\n",
       "      <td>0.932689</td>\n",
       "      <td>0.934027</td>\n",
       "      <td>0.932244</td>\n",
       "      <td>0.938187</td>\n",
       "      <td>0.934697</td>\n",
       "      <td>0.005539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.895584</td>\n",
       "      <td>0.173348</td>\n",
       "      <td>0.024930</td>\n",
       "      <td>0.011335</td>\n",
       "      <td>{0: 1, 1: 10.827586206896552}</td>\n",
       "      <td>{'class_weight': {0: 1, 1: 10.827586206896552}}</td>\n",
       "      <td>0.156495</td>\n",
       "      <td>0.154939</td>\n",
       "      <td>0.156243</td>\n",
       "      <td>0.153795</td>\n",
       "      <td>...</td>\n",
       "      <td>0.964933</td>\n",
       "      <td>0.937444</td>\n",
       "      <td>0.940119</td>\n",
       "      <td>0.960773</td>\n",
       "      <td>0.949331</td>\n",
       "      <td>0.950966</td>\n",
       "      <td>0.948291</td>\n",
       "      <td>0.955572</td>\n",
       "      <td>0.948947</td>\n",
       "      <td>0.008999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.909905</td>\n",
       "      <td>0.239815</td>\n",
       "      <td>0.022739</td>\n",
       "      <td>0.012164</td>\n",
       "      <td>{0: 1, 1: 11.482758620689655}</td>\n",
       "      <td>{'class_weight': {0: 1, 1: 11.482758620689655}}</td>\n",
       "      <td>0.157071</td>\n",
       "      <td>0.155172</td>\n",
       "      <td>0.153022</td>\n",
       "      <td>0.153310</td>\n",
       "      <td>...</td>\n",
       "      <td>0.976820</td>\n",
       "      <td>0.947103</td>\n",
       "      <td>0.948440</td>\n",
       "      <td>0.978306</td>\n",
       "      <td>0.966122</td>\n",
       "      <td>0.968945</td>\n",
       "      <td>0.966122</td>\n",
       "      <td>0.971768</td>\n",
       "      <td>0.963865</td>\n",
       "      <td>0.010543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.928159</td>\n",
       "      <td>0.166376</td>\n",
       "      <td>0.023338</td>\n",
       "      <td>0.009463</td>\n",
       "      <td>{0: 1, 1: 12.137931034482758}</td>\n",
       "      <td>{'class_weight': {0: 1, 1: 12.137931034482758}}</td>\n",
       "      <td>0.156067</td>\n",
       "      <td>0.154564</td>\n",
       "      <td>0.150951</td>\n",
       "      <td>0.153188</td>\n",
       "      <td>...</td>\n",
       "      <td>0.983655</td>\n",
       "      <td>0.959584</td>\n",
       "      <td>0.958395</td>\n",
       "      <td>0.987667</td>\n",
       "      <td>0.977266</td>\n",
       "      <td>0.981129</td>\n",
       "      <td>0.977117</td>\n",
       "      <td>0.981724</td>\n",
       "      <td>0.974845</td>\n",
       "      <td>0.009578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.962514</td>\n",
       "      <td>0.208870</td>\n",
       "      <td>0.028225</td>\n",
       "      <td>0.013171</td>\n",
       "      <td>{0: 1, 1: 12.793103448275861}</td>\n",
       "      <td>{'class_weight': {0: 1, 1: 12.793103448275861}}</td>\n",
       "      <td>0.155046</td>\n",
       "      <td>0.152062</td>\n",
       "      <td>0.149072</td>\n",
       "      <td>0.153300</td>\n",
       "      <td>...</td>\n",
       "      <td>0.987370</td>\n",
       "      <td>0.979049</td>\n",
       "      <td>0.973403</td>\n",
       "      <td>0.992422</td>\n",
       "      <td>0.985736</td>\n",
       "      <td>0.987667</td>\n",
       "      <td>0.984695</td>\n",
       "      <td>0.987667</td>\n",
       "      <td>0.983745</td>\n",
       "      <td>0.005468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1.020226</td>\n",
       "      <td>0.251451</td>\n",
       "      <td>0.021142</td>\n",
       "      <td>0.009073</td>\n",
       "      <td>{0: 1, 1: 13.448275862068964}</td>\n",
       "      <td>{'class_weight': {0: 1, 1: 13.448275862068964}}</td>\n",
       "      <td>0.154074</td>\n",
       "      <td>0.149864</td>\n",
       "      <td>0.148270</td>\n",
       "      <td>0.152183</td>\n",
       "      <td>...</td>\n",
       "      <td>0.990936</td>\n",
       "      <td>0.989302</td>\n",
       "      <td>0.991828</td>\n",
       "      <td>0.995394</td>\n",
       "      <td>0.989302</td>\n",
       "      <td>0.991679</td>\n",
       "      <td>0.988410</td>\n",
       "      <td>0.991085</td>\n",
       "      <td>0.989867</td>\n",
       "      <td>0.002974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.948432</td>\n",
       "      <td>0.130857</td>\n",
       "      <td>0.020146</td>\n",
       "      <td>0.007851</td>\n",
       "      <td>{0: 1, 1: 14.103448275862068}</td>\n",
       "      <td>{'class_weight': {0: 1, 1: 14.103448275862068}}</td>\n",
       "      <td>0.152983</td>\n",
       "      <td>0.149214</td>\n",
       "      <td>0.148103</td>\n",
       "      <td>0.150829</td>\n",
       "      <td>...</td>\n",
       "      <td>0.991976</td>\n",
       "      <td>0.993611</td>\n",
       "      <td>0.998068</td>\n",
       "      <td>0.997177</td>\n",
       "      <td>0.993016</td>\n",
       "      <td>0.994205</td>\n",
       "      <td>0.992273</td>\n",
       "      <td>0.994354</td>\n",
       "      <td>0.993284</td>\n",
       "      <td>0.002831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.820171</td>\n",
       "      <td>0.186800</td>\n",
       "      <td>0.019847</td>\n",
       "      <td>0.010028</td>\n",
       "      <td>{0: 1, 1: 14.758620689655173}</td>\n",
       "      <td>{'class_weight': {0: 1, 1: 14.758620689655173}}</td>\n",
       "      <td>0.152389</td>\n",
       "      <td>0.149024</td>\n",
       "      <td>0.148465</td>\n",
       "      <td>0.150211</td>\n",
       "      <td>...</td>\n",
       "      <td>0.992571</td>\n",
       "      <td>0.995542</td>\n",
       "      <td>0.999257</td>\n",
       "      <td>0.998217</td>\n",
       "      <td>0.994799</td>\n",
       "      <td>0.995542</td>\n",
       "      <td>0.994502</td>\n",
       "      <td>0.995691</td>\n",
       "      <td>0.994904</td>\n",
       "      <td>0.002475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1.053355</td>\n",
       "      <td>0.180486</td>\n",
       "      <td>0.027137</td>\n",
       "      <td>0.013014</td>\n",
       "      <td>{0: 1, 1: 15.413793103448276}</td>\n",
       "      <td>{'class_weight': {0: 1, 1: 15.413793103448276}}</td>\n",
       "      <td>0.151605</td>\n",
       "      <td>0.148762</td>\n",
       "      <td>0.149169</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.993759</td>\n",
       "      <td>0.997474</td>\n",
       "      <td>0.999554</td>\n",
       "      <td>0.998811</td>\n",
       "      <td>0.996434</td>\n",
       "      <td>0.996285</td>\n",
       "      <td>0.996285</td>\n",
       "      <td>0.996285</td>\n",
       "      <td>0.996107</td>\n",
       "      <td>0.002123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1.028170</td>\n",
       "      <td>0.216242</td>\n",
       "      <td>0.020545</td>\n",
       "      <td>0.007943</td>\n",
       "      <td>{0: 1, 1: 16.06896551724138}</td>\n",
       "      <td>{'class_weight': {0: 1, 1: 16.06896551724138}}</td>\n",
       "      <td>0.150479</td>\n",
       "      <td>0.148533</td>\n",
       "      <td>0.149121</td>\n",
       "      <td>0.149789</td>\n",
       "      <td>...</td>\n",
       "      <td>0.994948</td>\n",
       "      <td>0.998960</td>\n",
       "      <td>0.999851</td>\n",
       "      <td>0.998960</td>\n",
       "      <td>0.997028</td>\n",
       "      <td>0.997623</td>\n",
       "      <td>0.997028</td>\n",
       "      <td>0.997474</td>\n",
       "      <td>0.997132</td>\n",
       "      <td>0.001758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1.057425</td>\n",
       "      <td>0.217119</td>\n",
       "      <td>0.022654</td>\n",
       "      <td>0.010785</td>\n",
       "      <td>{0: 1, 1: 16.724137931034484}</td>\n",
       "      <td>{'class_weight': {0: 1, 1: 16.724137931034484}}</td>\n",
       "      <td>0.150371</td>\n",
       "      <td>0.148668</td>\n",
       "      <td>0.149597</td>\n",
       "      <td>0.149669</td>\n",
       "      <td>...</td>\n",
       "      <td>0.995988</td>\n",
       "      <td>0.998960</td>\n",
       "      <td>0.999851</td>\n",
       "      <td>0.999108</td>\n",
       "      <td>0.998217</td>\n",
       "      <td>0.998514</td>\n",
       "      <td>0.997920</td>\n",
       "      <td>0.998366</td>\n",
       "      <td>0.997786</td>\n",
       "      <td>0.001501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.948722</td>\n",
       "      <td>0.169689</td>\n",
       "      <td>0.023339</td>\n",
       "      <td>0.010204</td>\n",
       "      <td>{0: 1, 1: 17.379310344827587}</td>\n",
       "      <td>{'class_weight': {0: 1, 1: 17.379310344827587}}</td>\n",
       "      <td>0.149550</td>\n",
       "      <td>0.148997</td>\n",
       "      <td>0.149446</td>\n",
       "      <td>0.149720</td>\n",
       "      <td>...</td>\n",
       "      <td>0.996731</td>\n",
       "      <td>0.999108</td>\n",
       "      <td>0.999851</td>\n",
       "      <td>0.999108</td>\n",
       "      <td>0.998514</td>\n",
       "      <td>0.998514</td>\n",
       "      <td>0.998366</td>\n",
       "      <td>0.998514</td>\n",
       "      <td>0.998143</td>\n",
       "      <td>0.001178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1.001771</td>\n",
       "      <td>0.174907</td>\n",
       "      <td>0.021246</td>\n",
       "      <td>0.007462</td>\n",
       "      <td>{0: 1, 1: 18.03448275862069}</td>\n",
       "      <td>{'class_weight': {0: 1, 1: 18.03448275862069}}</td>\n",
       "      <td>0.149634</td>\n",
       "      <td>0.149302</td>\n",
       "      <td>0.149266</td>\n",
       "      <td>0.149720</td>\n",
       "      <td>...</td>\n",
       "      <td>0.997474</td>\n",
       "      <td>0.999257</td>\n",
       "      <td>0.999851</td>\n",
       "      <td>0.999406</td>\n",
       "      <td>0.998514</td>\n",
       "      <td>0.998514</td>\n",
       "      <td>0.998514</td>\n",
       "      <td>0.998514</td>\n",
       "      <td>0.998395</td>\n",
       "      <td>0.000946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.912554</td>\n",
       "      <td>0.167891</td>\n",
       "      <td>0.018023</td>\n",
       "      <td>0.003934</td>\n",
       "      <td>{0: 1, 1: 18.689655172413794}</td>\n",
       "      <td>{'class_weight': {0: 1, 1: 18.689655172413794}}</td>\n",
       "      <td>0.149442</td>\n",
       "      <td>0.149263</td>\n",
       "      <td>0.149176</td>\n",
       "      <td>0.149660</td>\n",
       "      <td>...</td>\n",
       "      <td>0.997474</td>\n",
       "      <td>0.999257</td>\n",
       "      <td>0.999851</td>\n",
       "      <td>0.999406</td>\n",
       "      <td>0.998663</td>\n",
       "      <td>0.998663</td>\n",
       "      <td>0.998663</td>\n",
       "      <td>0.998663</td>\n",
       "      <td>0.998484</td>\n",
       "      <td>0.000911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.956652</td>\n",
       "      <td>0.252737</td>\n",
       "      <td>0.021393</td>\n",
       "      <td>0.007390</td>\n",
       "      <td>{0: 1, 1: 19.344827586206897}</td>\n",
       "      <td>{'class_weight': {0: 1, 1: 19.344827586206897}}</td>\n",
       "      <td>0.149574</td>\n",
       "      <td>0.149486</td>\n",
       "      <td>0.149086</td>\n",
       "      <td>0.149660</td>\n",
       "      <td>...</td>\n",
       "      <td>0.997623</td>\n",
       "      <td>0.999257</td>\n",
       "      <td>0.999851</td>\n",
       "      <td>0.999406</td>\n",
       "      <td>0.998811</td>\n",
       "      <td>0.998960</td>\n",
       "      <td>0.998811</td>\n",
       "      <td>0.998811</td>\n",
       "      <td>0.998648</td>\n",
       "      <td>0.000801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1.040955</td>\n",
       "      <td>0.244945</td>\n",
       "      <td>0.023138</td>\n",
       "      <td>0.011290</td>\n",
       "      <td>{0: 1, 1: 20.0}</td>\n",
       "      <td>{'class_weight': {0: 1, 1: 20.0}}</td>\n",
       "      <td>0.149535</td>\n",
       "      <td>0.149395</td>\n",
       "      <td>0.149056</td>\n",
       "      <td>0.149660</td>\n",
       "      <td>...</td>\n",
       "      <td>0.997623</td>\n",
       "      <td>0.999406</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.999406</td>\n",
       "      <td>0.998960</td>\n",
       "      <td>0.999108</td>\n",
       "      <td>0.998960</td>\n",
       "      <td>0.998960</td>\n",
       "      <td>0.998782</td>\n",
       "      <td>0.000786</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>30 rows × 56 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "0        0.761607      0.145236         0.019650        0.004624   \n",
       "1        0.617786      0.175277         0.017653        0.003813   \n",
       "2        0.874075      0.193313         0.019148        0.006193   \n",
       "3        0.912075      0.197343         0.023637        0.008079   \n",
       "4        0.881650      0.186410         0.024535        0.009966   \n",
       "5        1.008922      0.230364         0.028424        0.013654   \n",
       "6        0.907042      0.145782         0.020148        0.006383   \n",
       "7        0.904527      0.149949         0.019249        0.004528   \n",
       "8        0.903327      0.139174         0.028009        0.009290   \n",
       "9        0.851733      0.091967         0.017154        0.004342   \n",
       "10       0.950211      0.199103         0.018951        0.005815   \n",
       "11       0.972725      0.204311         0.023537        0.012053   \n",
       "12       0.992364      0.257309         0.021142        0.006846   \n",
       "13       1.043197      0.160256         0.021398        0.010140   \n",
       "14       0.860723      0.178716         0.022839        0.009036   \n",
       "15       0.895584      0.173348         0.024930        0.011335   \n",
       "16       0.909905      0.239815         0.022739        0.012164   \n",
       "17       0.928159      0.166376         0.023338        0.009463   \n",
       "18       0.962514      0.208870         0.028225        0.013171   \n",
       "19       1.020226      0.251451         0.021142        0.009073   \n",
       "20       0.948432      0.130857         0.020146        0.007851   \n",
       "21       0.820171      0.186800         0.019847        0.010028   \n",
       "22       1.053355      0.180486         0.027137        0.013014   \n",
       "23       1.028170      0.216242         0.020545        0.007943   \n",
       "24       1.057425      0.217119         0.022654        0.010785   \n",
       "25       0.948722      0.169689         0.023339        0.010204   \n",
       "26       1.001771      0.174907         0.021246        0.007462   \n",
       "27       0.912554      0.167891         0.018023        0.003934   \n",
       "28       0.956652      0.252737         0.021393        0.007390   \n",
       "29       1.040955      0.244945         0.023138        0.011290   \n",
       "\n",
       "               param_class_weight  \\\n",
       "0                  {0: 1, 1: 1.0}   \n",
       "1   {0: 1, 1: 1.6551724137931034}   \n",
       "2    {0: 1, 1: 2.310344827586207}   \n",
       "3   {0: 1, 1: 2.9655172413793105}   \n",
       "4   {0: 1, 1: 3.6206896551724137}   \n",
       "5    {0: 1, 1: 4.275862068965517}   \n",
       "6    {0: 1, 1: 4.931034482758621}   \n",
       "7    {0: 1, 1: 5.586206896551724}   \n",
       "8    {0: 1, 1: 6.241379310344827}   \n",
       "9    {0: 1, 1: 6.896551724137931}   \n",
       "10   {0: 1, 1: 7.551724137931034}   \n",
       "11   {0: 1, 1: 8.206896551724139}   \n",
       "12   {0: 1, 1: 8.862068965517242}   \n",
       "13   {0: 1, 1: 9.517241379310345}   \n",
       "14  {0: 1, 1: 10.172413793103448}   \n",
       "15  {0: 1, 1: 10.827586206896552}   \n",
       "16  {0: 1, 1: 11.482758620689655}   \n",
       "17  {0: 1, 1: 12.137931034482758}   \n",
       "18  {0: 1, 1: 12.793103448275861}   \n",
       "19  {0: 1, 1: 13.448275862068964}   \n",
       "20  {0: 1, 1: 14.103448275862068}   \n",
       "21  {0: 1, 1: 14.758620689655173}   \n",
       "22  {0: 1, 1: 15.413793103448276}   \n",
       "23   {0: 1, 1: 16.06896551724138}   \n",
       "24  {0: 1, 1: 16.724137931034484}   \n",
       "25  {0: 1, 1: 17.379310344827587}   \n",
       "26   {0: 1, 1: 18.03448275862069}   \n",
       "27  {0: 1, 1: 18.689655172413794}   \n",
       "28  {0: 1, 1: 19.344827586206897}   \n",
       "29                {0: 1, 1: 20.0}   \n",
       "\n",
       "                                             params  split0_test_precision  \\\n",
       "0                  {'class_weight': {0: 1, 1: 1.0}}               0.000000   \n",
       "1   {'class_weight': {0: 1, 1: 1.6551724137931034}}               0.000000   \n",
       "2    {'class_weight': {0: 1, 1: 2.310344827586207}}               0.375000   \n",
       "3   {'class_weight': {0: 1, 1: 2.9655172413793105}}               0.161290   \n",
       "4   {'class_weight': {0: 1, 1: 3.6206896551724137}}               0.163142   \n",
       "5    {'class_weight': {0: 1, 1: 4.275862068965517}}               0.158451   \n",
       "6    {'class_weight': {0: 1, 1: 4.931034482758621}}               0.157895   \n",
       "7    {'class_weight': {0: 1, 1: 5.586206896551724}}               0.158498   \n",
       "8    {'class_weight': {0: 1, 1: 6.241379310344827}}               0.159193   \n",
       "9    {'class_weight': {0: 1, 1: 6.896551724137931}}               0.162146   \n",
       "10   {'class_weight': {0: 1, 1: 7.551724137931034}}               0.166584   \n",
       "11   {'class_weight': {0: 1, 1: 8.206896551724139}}               0.164053   \n",
       "12   {'class_weight': {0: 1, 1: 8.862068965517242}}               0.161142   \n",
       "13   {'class_weight': {0: 1, 1: 9.517241379310345}}               0.156605   \n",
       "14  {'class_weight': {0: 1, 1: 10.172413793103448}}               0.157895   \n",
       "15  {'class_weight': {0: 1, 1: 10.827586206896552}}               0.156495   \n",
       "16  {'class_weight': {0: 1, 1: 11.482758620689655}}               0.157071   \n",
       "17  {'class_weight': {0: 1, 1: 12.137931034482758}}               0.156067   \n",
       "18  {'class_weight': {0: 1, 1: 12.793103448275861}}               0.155046   \n",
       "19  {'class_weight': {0: 1, 1: 13.448275862068964}}               0.154074   \n",
       "20  {'class_weight': {0: 1, 1: 14.103448275862068}}               0.152983   \n",
       "21  {'class_weight': {0: 1, 1: 14.758620689655173}}               0.152389   \n",
       "22  {'class_weight': {0: 1, 1: 15.413793103448276}}               0.151605   \n",
       "23   {'class_weight': {0: 1, 1: 16.06896551724138}}               0.150479   \n",
       "24  {'class_weight': {0: 1, 1: 16.724137931034484}}               0.150371   \n",
       "25  {'class_weight': {0: 1, 1: 17.379310344827587}}               0.149550   \n",
       "26   {'class_weight': {0: 1, 1: 18.03448275862069}}               0.149634   \n",
       "27  {'class_weight': {0: 1, 1: 18.689655172413794}}               0.149442   \n",
       "28  {'class_weight': {0: 1, 1: 19.344827586206897}}               0.149574   \n",
       "29                {'class_weight': {0: 1, 1: 20.0}}               0.149535   \n",
       "\n",
       "    split1_test_precision  split2_test_precision  split3_test_precision  ...  \\\n",
       "0                0.000000               0.000000               0.000000  ...   \n",
       "1                0.000000               0.000000               0.000000  ...   \n",
       "2                0.000000               0.000000               0.304348  ...   \n",
       "3                0.325000               0.288889               0.236287  ...   \n",
       "4                0.237288               0.226601               0.177436  ...   \n",
       "5                0.229457               0.295688               0.170423  ...   \n",
       "6                0.235052               0.266537               0.174094  ...   \n",
       "7                0.210526               0.237172               0.171818  ...   \n",
       "8                0.198522               0.219523               0.168973  ...   \n",
       "9                0.185014               0.201855               0.165546  ...   \n",
       "10               0.170968               0.185162               0.162959  ...   \n",
       "11               0.161616               0.170384               0.156173  ...   \n",
       "12               0.158779               0.165002               0.153792  ...   \n",
       "13               0.157830               0.162785               0.154265  ...   \n",
       "14               0.157023               0.158928               0.154208  ...   \n",
       "15               0.154939               0.156243               0.153795  ...   \n",
       "16               0.155172               0.153022               0.153310  ...   \n",
       "17               0.154564               0.150951               0.153188  ...   \n",
       "18               0.152062               0.149072               0.153300  ...   \n",
       "19               0.149864               0.148270               0.152183  ...   \n",
       "20               0.149214               0.148103               0.150829  ...   \n",
       "21               0.149024               0.148465               0.150211  ...   \n",
       "22               0.148762               0.149169               0.150000  ...   \n",
       "23               0.148533               0.149121               0.149789  ...   \n",
       "24               0.148668               0.149597               0.149669  ...   \n",
       "25               0.148997               0.149446               0.149720  ...   \n",
       "26               0.149302               0.149266               0.149720  ...   \n",
       "27               0.149263               0.149176               0.149660  ...   \n",
       "28               0.149486               0.149086               0.149660  ...   \n",
       "29               0.149395               0.149056               0.149660  ...   \n",
       "\n",
       "    split2_train_recall_score  split3_train_recall_score  \\\n",
       "0                    0.000000                   0.000000   \n",
       "1                    0.000000                   0.000000   \n",
       "2                    0.009955                   0.018128   \n",
       "3                    0.063001                   0.091233   \n",
       "4                    0.189153                   0.251857   \n",
       "5                    0.316048                   0.357949   \n",
       "6                    0.454532                   0.449926   \n",
       "7                    0.592719                   0.572511   \n",
       "8                    0.680089                   0.672957   \n",
       "9                    0.758841                   0.726449   \n",
       "10                   0.829421                   0.798811   \n",
       "11                   0.880832                   0.862110   \n",
       "12                   0.907132                   0.900297   \n",
       "13                   0.926152                   0.921545   \n",
       "14                   0.943982                   0.931055   \n",
       "15                   0.964933                   0.937444   \n",
       "16                   0.976820                   0.947103   \n",
       "17                   0.983655                   0.959584   \n",
       "18                   0.987370                   0.979049   \n",
       "19                   0.990936                   0.989302   \n",
       "20                   0.991976                   0.993611   \n",
       "21                   0.992571                   0.995542   \n",
       "22                   0.993759                   0.997474   \n",
       "23                   0.994948                   0.998960   \n",
       "24                   0.995988                   0.998960   \n",
       "25                   0.996731                   0.999108   \n",
       "26                   0.997474                   0.999257   \n",
       "27                   0.997474                   0.999257   \n",
       "28                   0.997623                   0.999257   \n",
       "29                   0.997623                   0.999406   \n",
       "\n",
       "    split4_train_recall_score  split5_train_recall_score  \\\n",
       "0                    0.000000                   0.000000   \n",
       "1                    0.000000                   0.000000   \n",
       "2                    0.000000                   0.004160   \n",
       "3                    0.054532                   0.050817   \n",
       "4                    0.223477                   0.172660   \n",
       "5                    0.349777                   0.322883   \n",
       "6                    0.438187                   0.428678   \n",
       "7                    0.583358                   0.570728   \n",
       "8                    0.671174                   0.668648   \n",
       "9                    0.754829                   0.741753   \n",
       "10                   0.838336                   0.821842   \n",
       "11                   0.871322                   0.881872   \n",
       "12                   0.897771                   0.911887   \n",
       "13                   0.920654                   0.930163   \n",
       "14                   0.931204                   0.943685   \n",
       "15                   0.940119                   0.960773   \n",
       "16                   0.948440                   0.978306   \n",
       "17                   0.958395                   0.987667   \n",
       "18                   0.973403                   0.992422   \n",
       "19                   0.991828                   0.995394   \n",
       "20                   0.998068                   0.997177   \n",
       "21                   0.999257                   0.998217   \n",
       "22                   0.999554                   0.998811   \n",
       "23                   0.999851                   0.998960   \n",
       "24                   0.999851                   0.999108   \n",
       "25                   0.999851                   0.999108   \n",
       "26                   0.999851                   0.999406   \n",
       "27                   0.999851                   0.999406   \n",
       "28                   0.999851                   0.999406   \n",
       "29                   1.000000                   0.999406   \n",
       "\n",
       "    split6_train_recall_score  split7_train_recall_score  \\\n",
       "0                    0.000000                   0.000000   \n",
       "1                    0.000000                   0.000000   \n",
       "2                    0.020208                   0.016790   \n",
       "3                    0.085587                   0.079643   \n",
       "4                    0.235661                   0.223477   \n",
       "5                    0.355126                   0.349331   \n",
       "6                    0.465230                   0.463150   \n",
       "7                    0.575632                   0.572660   \n",
       "8                    0.659733                   0.663150   \n",
       "9                    0.728678                   0.727786   \n",
       "10                   0.804012                   0.802080   \n",
       "11                   0.858692                   0.859287   \n",
       "12                   0.893908                   0.900000   \n",
       "13                   0.913967                   0.917979   \n",
       "14                   0.932689                   0.934027   \n",
       "15                   0.949331                   0.950966   \n",
       "16                   0.966122                   0.968945   \n",
       "17                   0.977266                   0.981129   \n",
       "18                   0.985736                   0.987667   \n",
       "19                   0.989302                   0.991679   \n",
       "20                   0.993016                   0.994205   \n",
       "21                   0.994799                   0.995542   \n",
       "22                   0.996434                   0.996285   \n",
       "23                   0.997028                   0.997623   \n",
       "24                   0.998217                   0.998514   \n",
       "25                   0.998514                   0.998514   \n",
       "26                   0.998514                   0.998514   \n",
       "27                   0.998663                   0.998663   \n",
       "28                   0.998811                   0.998960   \n",
       "29                   0.998960                   0.999108   \n",
       "\n",
       "    split8_train_recall_score  split9_train_recall_score  \\\n",
       "0                    0.000000                   0.000000   \n",
       "1                    0.000000                   0.000000   \n",
       "2                    0.021694                   0.011887   \n",
       "3                    0.088410                   0.071471   \n",
       "4                    0.233581                   0.215156   \n",
       "5                    0.353343                   0.338336   \n",
       "6                    0.464487                   0.454978   \n",
       "7                    0.576672                   0.573700   \n",
       "8                    0.661070                   0.657801   \n",
       "9                    0.731947                   0.737741   \n",
       "10                   0.804012                   0.815750   \n",
       "11                   0.857504                   0.867905   \n",
       "12                   0.893611                   0.898811   \n",
       "13                   0.913967                   0.919316   \n",
       "14                   0.932244                   0.938187   \n",
       "15                   0.948291                   0.955572   \n",
       "16                   0.966122                   0.971768   \n",
       "17                   0.977117                   0.981724   \n",
       "18                   0.984695                   0.987667   \n",
       "19                   0.988410                   0.991085   \n",
       "20                   0.992273                   0.994354   \n",
       "21                   0.994502                   0.995691   \n",
       "22                   0.996285                   0.996285   \n",
       "23                   0.997028                   0.997474   \n",
       "24                   0.997920                   0.998366   \n",
       "25                   0.998366                   0.998514   \n",
       "26                   0.998514                   0.998514   \n",
       "27                   0.998663                   0.998663   \n",
       "28                   0.998811                   0.998811   \n",
       "29                   0.998960                   0.998960   \n",
       "\n",
       "    mean_train_recall_score  std_train_recall_score  \n",
       "0                  0.000000                0.000000  \n",
       "1                  0.000000                0.000000  \n",
       "2                  0.014724                0.007844  \n",
       "3                  0.077040                0.016527  \n",
       "4                  0.221182                0.024179  \n",
       "5                  0.345413                0.015530  \n",
       "6                  0.457995                0.015775  \n",
       "7                  0.580086                0.008402  \n",
       "8                  0.669638                0.008789  \n",
       "9                  0.738641                0.010877  \n",
       "10                 0.812428                0.013328  \n",
       "11                 0.866527                0.009062  \n",
       "12                 0.899082                0.006172  \n",
       "13                 0.919572                0.005274  \n",
       "14                 0.934697                0.005539  \n",
       "15                 0.948947                0.008999  \n",
       "16                 0.963865                0.010543  \n",
       "17                 0.974845                0.009578  \n",
       "18                 0.983745                0.005468  \n",
       "19                 0.989867                0.002974  \n",
       "20                 0.993284                0.002831  \n",
       "21                 0.994904                0.002475  \n",
       "22                 0.996107                0.002123  \n",
       "23                 0.997132                0.001758  \n",
       "24                 0.997786                0.001501  \n",
       "25                 0.998143                0.001178  \n",
       "26                 0.998395                0.000946  \n",
       "27                 0.998484                0.000911  \n",
       "28                 0.998648                0.000801  \n",
       "29                 0.998782                0.000786  \n",
       "\n",
       "[30 rows x 56 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(grid.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cadfeef5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1000x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pylab as plt\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "df = pd.DataFrame(grid.cv_results_)\n",
    "for score in ['mean_test_recall_score', 'mean_test_precision']:\n",
    "    plt.plot([_[1] for _ in df['param_class_weight']],\n",
    "            df[score],\n",
    "            label=score)\n",
    "    \n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f25dc4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
